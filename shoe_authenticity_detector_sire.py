# -*- coding: utf-8 -*-
"""Shoe Authenticity Detector - Sire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uye6gAfNNJG-o0l3naLvlYslcw5O5ypn
"""


path = '/content/drive/MyDrive/shoedataset'
import requests

def download_file_from_google_drive(id, destination):
    URL = path

    session = requests.Session()

    response = session.get(URL, params = { 'id' : id }, stream = True)
    token = get_confirm_token(response)

    if token:
        params = { 'id' : id, 'confirm' : token }
        response = session.get(URL, params = params, stream = True)

    save_response_content(response, destination)    

def get_confirm_token(response):
    for key, value in response.cookies.items():
        if key.startswith('download_warning'):
            return value

    return None

def save_response_content(response, destination):
    CHUNK_SIZE = 32768

    with open(destination, "wb") as f:
        for chunk in response.iter_content(CHUNK_SIZE):
            if chunk: # filter out keep-alive new chunks
                f.write(chunk)

if __name__ == "__main__":
    file_id = 'TAKE ID FROM SHAREABLE LINK'
    destination = 'DESTINATION FILE ON YOUR DISK'
    download_file_from_google_drive(file_id, destination)
#processing images


"""created by Julia Huang"""

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import pathlib
import cv2
import glob
import streamlit as st
from joblib import load
import numpy as np
import cv2
from datetime import datetime
from keras.models import load_model
import streamlit as st
import numpy as np
import glob
import pandas as pd
import tensorflow as tf
from PIL import Image, ImageOps
import keras
from datetime import date
from st_btn_select import st_btn_select

#processing images
real_path = path+ '/Real/*.png'
fake_path = path+ '/Fake/*.png'


#all images are 227x227 in RGB so 227, 227, 3
real_images = [cv2.imread(image) for image in glob.glob(real_path)]
fake_images = [cv2.imread(image) for image in glob.glob(fake_path)]

real_images

train_dataset = tf.keras.utils.image_dataset_from_directory(
    path,
    validation_split = 0.2,
    subset = "training",
    seed = 24,  
    image_size = (227, 227),
    batch_size = 32
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    path,
    validation_split = 0.2,
    subset = "validation",
    seed = 24,  
    image_size = (227, 227),
    batch_size = 32
)

train_dataset = train_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)

model = Sequential ([
    layers.Rescaling(1./255, input_shape = (227, 227, 3)),
    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation = 'relu'),
    layers.Dense(2)
])

model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])

model.fit (
    train_dataset, 
    validation_data = val_dataset,
    epochs = 70
)
model.save('model.h5')

loss = pd.DataFrame(model.history.history)

#plotting the loss and accuracy 
plt.figure(figsize=(10,10))

plt.subplot(2,2,1)
plt.plot(loss["loss"], label ="Loss")
plt.plot(loss["val_loss"], label = "Validation_loss")
plt.legend()
plt.title("Training and Validation Loss")

plt.subplot(2,2,2)
plt.plot(loss['accuracy'],label = "Training Accuracy")
plt.plot(loss['val_accuracy'], label ="Validation_ Accuracy ")
plt.legend()
plt.title("Training-Validation Accuracy")

"""#streamlit"""
'''
!pip install -q streamlit

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p /drive/ngrok-ssh
# %cd /drive/ngrok-ssh
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip
!unzip -u ngrok-stable-linux-amd64.zip
!cp /drive/ngrok-ssh/ngrok /ngrok
!chmod +x /ngrok

!/ngrok authtoken 2Fud0KVvq9S56Na5IbL6NRnHZtZ_6KvuGdz8px9MHULrMeHWf

'''

from pyngrok import ngrok
#Publish Web App (Run this again whenever you make changes)
public_url = ngrok.connect(port='80')
print (public_url)


import tensorflow as tf

# ====== Save model ========
model.save("model.h5")

# ====== Load model ========
tf.keras.models.load_model("model.h5")

model = tf.keras.models.load_model("model.h5")

model.summary()

model.input_shape

import cv2
import numpy as np
f = st.file_uploader("Upload Image")



if f is not None: 
  file_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)
  image = cv2.imdecode(file_bytes, 1)
  st.image(image, channels="BGR")

with open("/content/drive/MyDrive/shoedataset/Fake/IMG_9368.PNG", "rb") as f:
  file_bytes = np.asarray(bytearray(f.read()))
  image = cv2.imdecode(file_bytes, 1)

print("Shape of image:")
print(image.shape)
print(f"Maximum value in the image: ")
print(image.max())
print("Minimum value in the image:")
print(image.min())

# Here we convert to RGB because our plotting function takes in RGB images
px.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

resized = cv2.resize(image, (227, 227), interpolation=cv2.INTER_LANCZOS4)
print(resized.shape)
px.imshow(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB))

gray_1d = np.mean(resized, axis=-1)
gray = np.zeros_like(resized)
gray[:,:,0] = gray_1d
gray[:,:,1] = gray_1d
gray[:,:,2] = gray_1d

px.imshow(gray)

normalized = gray/255
px.imshow(normalized)

print("Shape of image:")
print(normalized.shape)
print(f"Maximum value in the image: ")
print(normalized.max())
print("Minimum value in the image:")
print(normalized.min())

OPTIONS = ['Fake', 'Real']

model_input = np.expand_dims(normalized,0)
scores = model.predict(model_input)
scores

df = pd.DataFrame()
df["Options"] = OPTIONS
df["Scores"] = scores.flatten()
px.bar(df, x='Options', y='Scores', title="Model scores for each type")

prediction = OPTIONS[scores.argmax()]
print(prediction)

"""#combined method"""




# #@title Run this cell to initialize your website!

import streamlit as st
from joblib import load
import numpy as np
import cv2
from datetime import datetime
from keras.models import load_model
import streamlit as st
import numpy as np
import glob
import pandas as pd
import tensorflow as tf
from PIL import Image, ImageOps
import keras
from datetime import date
from st_btn_select import st_btn_select

selection = st_btn_select(('CHECK YOUR SHOES', 'ABOUT'))

if selection == 'CHECK YOUR SHOES':
    
    st.title("Shoethentic")
    st.header("Created by Julia Huang & Justin Huang")
    st.header("Detect if your shoes are fake or not via AI!")
    st.subheader("Quick and easy; you only need to upload images to receive an automatic result!")



    image = st.file_uploader(label = "Upload an image for analysis:", type = ['png', 'jpg', 'jpeg', 'tif', 'tiff', 'raw', 'webp'])

    def import_and_predict(image_data, model):
        size = (227, 227)
        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
        img = tf.keras.utils.img_to_array(image)
        img = tf.expand_dims(img, 0)
        probs = model.predict(img)
        score = tf.nn.softmax(probs[0])
        text = ("Shoethentic predicts that this is an image of **{} shoe with {:.2f}% confidence**."
        .format(class_names[np.argmax(score)], 100 * np.max(score)))
        return text

    loaded_model = tf.keras.models.load_model('model.h5')
    class_names = ['Fake', 'Real']

    predictionText = "Prediction: Waiting for an image upload"

    if image is not None:
        st.image(image)
        predictionText = (import_and_predict(Image.open(image), loaded_model))

    st.markdown(predictionText)    

    

if selection == 'ABOUT':
    st.title("About")
   
    st.subheader("About the Creator")
    st.markdown("Shoethentic's web app and model is built by Julia Huang, a current student and developer at Sire, and the dataset is created by Justin Huang.")

    st.subheader("Mission")
    st.markdown("Due to the high prevalence of counterfeit shoe production, the goal of **Shoethentic** is to provide the sneakerhead community an opportunity to check the authenticity of each and every shoe they buy. **Shoethentic** aims to make this checking process simpler and more convenient by utilizing AI & machine learning.")
    st.subheader("How Shoethentic was Built")
  
    st.markdown("Shoethentic has two parts: the AI model and web app. The AI model is built using the TensorFlow framework while the web app is built using Streamlit. We trained the model on a dataset consisting of fake and real shoe images sourced from the CheckCheck mobile app.")
 
    st.subheader("Future of Shoethentic")
    st.markdown("We plan to improve the accuracy of the AI model even more when checking for shoes and integrate it into the Sire website later on.")

from pyngrok import ngrok
#Publish Web App (Run this again whenever you make changes)
public_url = ngrok.connect(port='80')
print (public_url)

