# -*- coding: utf-8 -*-
"""Shoe Authenticity Detector - Sire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uye6gAfNNJG-o0l3naLvlYslcw5O5ypn
"""

from google.colab import drive
drive.mount('/content/drive')

#processing images
path = '/content/drive/MyDrive/shoedataset'

"""created by Julia Huang"""

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import pandas as pd
import numpy as np
import tensorflow as tf
import os
import matplotlib.pyplot as plt
import pathlib
import cv2
import glob

#processing images
real_path = path+ '/Real/*.png'
fake_path = path+ '/Fake/*.png'


#all images are 227x227 in RGB so 227, 227, 3
real_images = [cv2.imread(image) for image in glob.glob(real_path)]
fake_images = [cv2.imread(image) for image in glob.glob(fake_path)]

real_images

train_dataset = tf.keras.utils.image_dataset_from_directory(
    path,
    validation_split = 0.2,
    subset = "training",
    seed = 24,  
    image_size = (227, 227),
    batch_size = 32
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    path,
    validation_split = 0.2,
    subset = "validation",
    seed = 24,  
    image_size = (227, 227),
    batch_size = 32
)

train_dataset = train_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().shuffle(13).prefetch(buffer_size = tf.data.AUTOTUNE)

model = Sequential ([
    layers.Rescaling(1./255, input_shape = (227, 227, 3)),
    layers.Conv2D(16, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation = 'relu'),
    layers.Dense(2)
])

model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])

model.fit (
    train_dataset, 
    validation_data = val_dataset,
    epochs = 70
)
model.save('model.h5')

loss = pd.DataFrame(model.history.history)

#plotting the loss and accuracy 
plt.figure(figsize=(10,10))

plt.subplot(2,2,1)
plt.plot(loss["loss"], label ="Loss")
plt.plot(loss["val_loss"], label = "Validation_loss")
plt.legend()
plt.title("Training and Validation Loss")

plt.subplot(2,2,2)
plt.plot(loss['accuracy'],label = "Training Accuracy")
plt.plot(loss['val_accuracy'], label ="Validation_ Accuracy ")
plt.legend()
plt.title("Training-Validation Accuracy")



"""#streamlit"""

! pip install -q streamlit

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p /drive/ngrok-ssh
# %cd /drive/ngrok-ssh
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip
!unzip -u ngrok-stable-linux-amd64.zip
!cp /drive/ngrok-ssh/ngrok /ngrok
!chmod +x /ngrok

!/ngrok authtoken 2Fud0KVvq9S56Na5IbL6NRnHZtZ_6KvuGdz8px9MHULrMeHWf

!pip install pyngrok

from pyngrok import ngrok 
public_url = ngrok.connect(port='8501')
public_url

!kill 8501

# Commented out IPython magic to ensure Python compatibility.
# %%writefile streamlit_app.py 
# import streamlit as st 
# st.markdown(""" This is a Streamlit App """)
# from datetime import datetime
# from keras.models import load_model
# import streamlit as st
# import numpy as np
# import glob
# import pandas as pd
# import tensorflow as tf
# from PIL import Image, ImageOps
# import keras
# from datetime import date
# from st_btn_select import st_btn_select
# 
# selection = st_btn_select(('SUBMIT', 'ABOUT'))
# 
# if selection == 'SUBMIT':
# 
#     st.title("Welcome to Shoethentic!")
#     st.subheader("Rallying together to gather and spread intel on the health of corals and provide coral reef organizations with up-to-date information using ML")
# 
#     image = st.file_uploader(label = "Upload an image for analysis:", type = ['png', 'jpg', 'jpeg', 'tif', 'tiff', 'raw', 'webp'])
# 
#     def import_and_predict(image_data, model):
#         size = (227, 227)
#         image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
#         img = tf.keras.utils.img_to_array(image)
#         img = tf.expand_dims(img, 0)
#         probs = model.predict(img)
#         score = tf.nn.softmax(probs[0])
#         text = ("Shoethentic predicts that this is an image of **{} shoe with {:.2f}% confidence**."
#         .format(class_names[np.argmax(score)], 100 * np.max(score)))
#         return text
# 
#     loaded_model = tf.keras.models.load_model('model.h5')
#     class_names = ['Fake', 'Real']
# 
#     predictionText = "Prediction: Waiting for an image upload"
# 
#     if image is not None:
#         st.image(image)
#         predictionText = (import_and_predict(Image.open(image), loaded_model))
# 
#     st.markdown(predictionText)    
# 
#     st.subheader("Location")
#     countries = ('Select a country', 'Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', "Côte d'Ivoire", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', "Korea, Democratic People's Republic of", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', "Lao People's Democratic Republic", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe')
#     states = ("Select a state", "Alaska", "Alabama", "Arkansas", "American Samoa", "Arizona", "California", "Colorado", "Connecticut", "District ", "of Columbia", "Delaware", "Florida", "Georgia", "Guam", "Hawaii", "Iowa", "Idaho", "Illinois", "Indiana", "Kansas", "Kentucky", "Louisiana", "Massachusetts", "Maryland", "Maine", "Michigan", "Minnesota", "Missouri", "Mississippi", "Montana", "North Carolina", "North Dakota", "Nebraska", "New Hampshire", "New Jersey", "New Mexico", "Nevada", "New York", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Puerto Rico", "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Virginia", "Virgin Islands", "Vermont", "Washington", "Wisconsin", "West Virginia", "Wyoming")
#     country = st.selectbox("Select the country where the image was taken:", countries)
#     if country == 'United States':
#         state = st.selectbox('Select the state where the image was taken:', states)
#     area = st.text_area("Best describe the area in which the image was taken (ex: city, diving spot, beach):")
# 
#     st.subheader("Date")
#     st.date_input("Select the date that the image was taken on:", max_value = date.today())
# 
#     st.subheader("Personal Information")
#     st.markdown("This section's main purpose is for when any receiving organizations have questions\non your submission and need to contact you")
#     last_name = st.text_input("Last Name:")
#     first_name = st.text_input("First Name:")
#     birth_date = st.date_input("Date of Birth:", min_value = date(1900, 1, 1), max_value = date.today())
#     email_address = st.text_input("Email Address:")
#     phone_number = st.text_input("Phone Number:")
#     submit = st.button("Submit")    
#     st.markdown("This button currently doesn't do anything, but once **Corally** partners with various coral reef organizations, it will submit all of this information to connected organizations.")
# 
# if selection == 'ABOUT':
#     st.title("About")
#     st.image("Corally Logo.png", width = 500)
#     st.subheader("Our Mission")
#     st.markdown("The goal of **Corally** is to provide the community an opportunity to **rally** together and gather information about the coral around them and forward it to coral reef organizations. **Corally** aims to make data easier to sort through and process by utilizing machine learning technology to help process submitted images before they reach the organization.")
#     st.subheader("How Corally was Built")
#     st.image("Corally_Slides.jpeg")
#     st.markdown("**Corally**'s web app was built using a framework called Streamlit, which is used to create and host web apps in Python. The model for determining the health of coral images was created using TensorFlow and Keras. This model is an 11-layer convolutional neural network with 9 hidden layers and 3 convolutional layers. It was trained on a dataset of dead, bleached, and healthy coral for 20 epochs.")
#     st.image("CNN.png")
#     st.subheader("The Future of Corally")
#     st.markdown("In the future, I hope to further optimize the TensorFlow model as it struggles with bleached versus dead coral images. It also has a strong bias towards healthy coral. I also plan to partner with as many coral reef organizations as possible in order to maximize the impact of **Corally**.")

!npm install localtunnel

!streamlit run /content/app.py &>/content/logs.txt &

!npx localtunnel --port 8501

!streamlit run /content/streamlit_app.py & npx localtunnel — port 8501



"""#inspirit method"""

#@title Run this cell to load some data
!pip -q install streamlit > /dev/null
!pip -q install pyngrok > /dev/null
!wget https://www.dropbox.com/s/072b5vf4b33bu1l/emotion_detection_model_for_streamlit.h5 > /dev/null
!wget https://www.dropbox.com/s/p52z1qle0x1uf6f/happy.png > /dev/null

!wget /content/model.h5 > /dev/null
!wget https://drive.google.com/drive/folders/1-H7smHkhS1g2R3d5pkPNt9RDq6S6ebFd > /dev/null
import tensorflow as tf
import pandas as pd
import cv2
import plotly.express as px
import numpy as np



#@title Run this to prepare our libraries and data! { display-mode: "form" } 

# networking
!pip -q install pyngrok
!pip -q install streamlit
!pip -q install patool

import cv2
import gdown
import glob
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import os
import patoolib

from joblib import dump
from pyngrok import ngrok
from tqdm import tqdm

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

DATA_ROOT = '/content/drive/MyDrive/shoedataset'
os.makedirs(DATA_ROOT, exist_ok=True)
max_samples = 3000
'''
blood_slide_url = 'https://drive.google.com/uc?id=1lffxAG8gykh1dh1pCP34uRkH3XMwuNt-'
blood_slide_path = os.path.join(DATA_ROOT, 'blood_slide.jpg')
gdown.download(blood_slide_url, blood_slide_path, True)
'''
malaria_imgs_url = 'https://drive.google.com/drive/folders/1-H7smHkhS1g2R3d5pkPNt9RDq6S6ebFd?usp=sharing'
malaria_imgs_path = os.path.join(DATA_ROOT)
gdown.download(malaria_imgs_url, malaria_imgs_path, True)

#if os.path.exists(os.path.join(DATA_ROOT, 'malaria_images')) == False:
  #patoolib.extract_archive(os.path.join(DATA_ROOT, 'malaria_imgs.zip'), outdir=DATA_ROOT)

print("Downloaded Data")

u_malaria_img_paths = glob.glob('/content/drive/MyDrive/shoedataset/Real/*png')
p_malaria_img_paths = glob.glob('/content/drive/MyDrive/shoedataset/Fake/*png')

NUM_SAMPLES = len(u_malaria_img_paths) + len(p_malaria_img_paths)

X = []
y = []

X_g = []

for i in tqdm(range(max_samples)):
  img = cv2.imread(u_malaria_img_paths[i])
  X.append(cv2.resize(img,(227,227)))

  gray_img = cv2.imread(u_malaria_img_paths[i],0)
  X_g.append(cv2.resize(gray_img,(227,227)))

  y.append(0)

for i in tqdm(range(max_samples)):
  img = cv2.imread(p_malaria_img_paths[i])
  X.append(cv2.resize(img,(227,227)))

  gray_img = cv2.imread(p_malaria_img_paths[i],0)
  X_g.append(cv2.resize(gray_img,(227,227)))

  y.append(1)

X = np.stack(X)
X_g = np.stack(X_g)
X_reshaped = np.reshape(X_g,(X_g.shape[0],2500))

y = np.array(y)

blood_samples_dir = 'blood_samples'
if (os.path.exists(blood_samples_dir) == False):
  os.mkdir(blood_samples_dir)

for i, img in enumerate(X[2995:3005]):
  plt.imsave('test_img_{}.jpg'.format(i), img)
  
print("Created our X and y variables")

dump(model, "model.joblib")

# Commented out IPython magic to ensure Python compatibility.
# #@title Run this cell to initialize your website!
# %%writefile app.py
# import streamlit as st
# from joblib import load
# import numpy as np
# import cv2
# 
# st.title("Shoethentic - created by Julia Huang from Sire")
# st.header("Detect if your shoes are fake or not via AI!")
# st.subheader("Quick and easy, as you only need to upload images to receive a result!")
# 
# model = load("model.h5" )
# uploaded_file = st.file_uploader("Upload Image")
# 
# if uploaded_file is not None:    
#     file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
#     image = cv2.imdecode(file_bytes, 1)
#     st.image(image, caption="Input Image", channels="BGR")
# 
#     small = cv2.resize(image, (227,227))
#     gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)
#   
#     gray_flat = np.reshape(gray,(1,2500))
#     prediction = model.predict(gray_flat)[0]
# 
#     if prediction == 1:
#       st.write('Real')
#     else:
#       st.write('Fake')

from pyngrok import ngrok
#Publish Web App (Run this again whenever you make changes)
public_url = ngrok.connect(port='80')
print (public_url)
! streamlit run --server.port 80 app.py

import tensorflow as tf

# ====== Save model ========
model.save("model.h5")

# ====== Load model ========
tf.keras.models.load_model("model.h5")

model = tf.keras.models.load_model("model.h5")

model.summary()

model.input_shape

import cv2
import numpy as np
f = st.file_uploader("Upload Image")

!streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py

if f is not None: 
  file_bytes = np.asarray(bytearray(f.read()), dtype=np.uint8)
  image = cv2.imdecode(file_bytes, 1)
  st.image(image, channels="BGR")

with open("/content/drive/MyDrive/shoedataset/Fake/IMG_9368.PNG", "rb") as f:
  file_bytes = np.asarray(bytearray(f.read()))
  image = cv2.imdecode(file_bytes, 1)

print("Shape of image:")
print(image.shape)
print(f"Maximum value in the image: ")
print(image.max())
print("Minimum value in the image:")
print(image.min())

# Here we convert to RGB because our plotting function takes in RGB images
px.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

resized = cv2.resize(image, (227, 227), interpolation=cv2.INTER_LANCZOS4)
print(resized.shape)
px.imshow(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB))

gray_1d = np.mean(resized, axis=-1)
gray = np.zeros_like(resized)
gray[:,:,0] = gray_1d
gray[:,:,1] = gray_1d
gray[:,:,2] = gray_1d

px.imshow(gray)

normalized = gray/255
px.imshow(normalized)

print("Shape of image:")
print(normalized.shape)
print(f"Maximum value in the image: ")
print(normalized.max())
print("Minimum value in the image:")
print(normalized.min())

OPTIONS = ['Fake', 'Real']

model_input = np.expand_dims(normalized,0)
scores = model.predict(model_input)
scores

df = pd.DataFrame()
  df["Options"] = OPTIONS
  df["Scores"] = scores.flatten()
  px.bar(df, x='Options', y='Scores', title="Model scores for each type")

prediction = OPTIONS[scores.argmax()]
print(prediction)

"""#combine coral+inspirit method"""

import streamlit as st 
st.markdown(""" This is a Streamlit App """)
from datetime import datetime
from keras.models import load_model
import streamlit as st
import numpy as np
import glob
import pandas as pd
import tensorflow as tf
from PIL import Image, ImageOps
import keras
from datetime import date
from st_btn_select import st_btn_select

selection = st_btn_select(('SUBMIT', 'ABOUT'))

if selection == 'SUBMIT':

    st.title("Welcome to Shoethentic!")
    st.subheader("Rallying together to gather and spread intel on the health of corals and provide coral reef organizations with up-to-date information using ML")

    image = st.file_uploader(label = "Upload an image for analysis:", type = ['png', 'jpg', 'jpeg', 'tif', 'tiff', 'raw', 'webp'])

    def import_and_predict(image_data, model):
        size = (227, 227)
        image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
        img = tf.keras.utils.img_to_array(image)
        img = tf.expand_dims(img, 0)
        probs = model.predict(img)
        score = tf.nn.softmax(probs[0])
        text = ("Shoethentic predicts that this is an image of **{} shoe with {:.2f}% confidence**."
        .format(class_names[np.argmax(score)], 100 * np.max(score)))
        return text

    loaded_model = tf.keras.models.load_model('model.h5')
    class_names = ['Fake', 'Real']

    predictionText = "Prediction: Waiting for an image upload"

    if image is not None:
        st.image(image)
        predictionText = (import_and_predict(Image.open(image), loaded_model))

    st.markdown(predictionText)    

    st.subheader("Location")
    countries = ('Select a country', 'Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', "Côte d'Ivoire", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', "Korea, Democratic People's Republic of", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', "Lao People's Democratic Republic", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe')
    states = ("Select a state", "Alaska", "Alabama", "Arkansas", "American Samoa", "Arizona", "California", "Colorado", "Connecticut", "District ", "of Columbia", "Delaware", "Florida", "Georgia", "Guam", "Hawaii", "Iowa", "Idaho", "Illinois", "Indiana", "Kansas", "Kentucky", "Louisiana", "Massachusetts", "Maryland", "Maine", "Michigan", "Minnesota", "Missouri", "Mississippi", "Montana", "North Carolina", "North Dakota", "Nebraska", "New Hampshire", "New Jersey", "New Mexico", "Nevada", "New York", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Puerto Rico", "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Virginia", "Virgin Islands", "Vermont", "Washington", "Wisconsin", "West Virginia", "Wyoming")
    country = st.selectbox("Select the country where the image was taken:", countries)
    if country == 'United States':
        state = st.selectbox('Select the state where the image was taken:', states)
    area = st.text_area("Best describe the area in which the image was taken (ex: city, diving spot, beach):")

    st.subheader("Date")
    st.date_input("Select the date that the image was taken on:", max_value = date.today())

    st.subheader("Personal Information")
    st.markdown("This section's main purpose is for when any receiving organizations have questions\non your submission and need to contact you")
    last_name = st.text_input("Last Name:")
    first_name = st.text_input("First Name:")
    birth_date = st.date_input("Date of Birth:", min_value = date(1900, 1, 1), max_value = date.today())
    email_address = st.text_input("Email Address:")
    phone_number = st.text_input("Phone Number:")
    submit = st.button("Submit")    
    st.markdown("This button currently doesn't do anything, but once **Corally** partners with various coral reef organizations, it will submit all of this information to connected organizations.")

if selection == 'ABOUT':
    st.title("About")
    st.image("Corally Logo.png", width = 500)
    st.subheader("Our Mission")
    st.markdown("The goal of **Corally** is to provide the community an opportunity to **rally** together and gather information about the coral around them and forward it to coral reef organizations. **Corally** aims to make data easier to sort through and process by utilizing machine learning technology to help process submitted images before they reach the organization.")
    st.subheader("How Corally was Built")
    st.image("Corally_Slides.jpeg")
    st.markdown("**Corally**'s web app was built using a framework called Streamlit, which is used to create and host web apps in Python. The model for determining the health of coral images was created using TensorFlow and Keras. This model is an 11-layer convolutional neural network with 9 hidden layers and 3 convolutional layers. It was trained on a dataset of dead, bleached, and healthy coral for 20 epochs.")
    st.image("CNN.png")
    st.subheader("The Future of Corally")
    st.markdown("In the future, I hope to further optimize the TensorFlow model as it struggles with bleached versus dead coral images. It also has a strong bias towards healthy coral. I also plan to partner with as many coral reef organizations as possible in order to maximize the impact of **Corally**.")

!pip install st-btn-select

# Commented out IPython magic to ensure Python compatibility.
# #@title Run this cell to initialize your website!
# %%writefile app.py
# import streamlit as st
# from joblib import load
# import numpy as np
# import cv2
# from datetime import datetime
# from keras.models import load_model
# import streamlit as st
# import numpy as np
# import glob
# import pandas as pd
# import tensorflow as tf
# from PIL import Image, ImageOps
# import keras
# from datetime import date
# from st_btn_select import st_btn_select
# 
# selection = st_btn_select(('CHECK YOUR SHOES', 'ABOUT'))
# 
# if selection == 'CHECK YOUR SHOES':
#     
#     st.title("Shoethentic")
#     st.header("Created by Julia Huang from Sire")
#     st.header("Detect if your shoes are fake or not via AI!")
#     st.subheader("Quick and easy; you only need to upload images to receive an automatic result!")
# 
# 
# 
#     image = st.file_uploader(label = "Upload an image for analysis:", type = ['png', 'jpg', 'jpeg', 'tif', 'tiff', 'raw', 'webp'])
# 
#     def import_and_predict(image_data, model):
#         size = (227, 227)
#         image = ImageOps.fit(image_data, size, Image.ANTIALIAS)
#         img = tf.keras.utils.img_to_array(image)
#         img = tf.expand_dims(img, 0)
#         probs = model.predict(img)
#         score = tf.nn.softmax(probs[0])
#         text = ("Shoethentic predicts that this is an image of **{} shoe with {:.2f}% confidence**."
#         .format(class_names[np.argmax(score)], 100 * np.max(score)))
#         return text
# 
#     loaded_model = tf.keras.models.load_model('model.h5')
#     class_names = ['Fake', 'Real']
# 
#     predictionText = "Prediction: Waiting for an image upload"
# 
#     if image is not None:
#         st.image(image)
#         predictionText = (import_and_predict(Image.open(image), loaded_model))
# 
#     st.markdown(predictionText)    
# 
#     
# 
# if selection == 'ABOUT':
#     st.title("About")
#    
#     st.subheader("About the Creator")
#     st.markdown("Shoethentic is built by Julia Huang, a current student and developer at Sire."
# 
#     st.subheader("Mission")
#     st.markdown("Due to the high prevalence of counterfeit shoe production, the goal of **Shoethentic** is to provide the sneakerhead community an opportunity to check the authenticity of each and every shoe they buy. **Shoethentic** aims to make this checking process simpler and more convenient by utilizing AI & machine learning.")
#     st.subheader("How Shoethentic was Built")
#   
#     st.markdown("Shoethentic has two parts: the AI model and web app. The AI model is built using the TensorFlow framework while the web app is built using Streamlit. We trained the model on a dataset consisting of fake and real shoe images sourced from the CheckCheck mobile app.")
#  
#     st.subheader("Future of Shoethentic")
#     st.markdown("We plan to improve the accuracy of the AI model even more when checking for shoes and integrate it into the Sire website later on.")

from pyngrok import ngrok
#Publish Web App (Run this again whenever you make changes)
public_url = ngrok.connect(port='80')
print (public_url)
! streamlit run --server.port 80 app.py

!git version